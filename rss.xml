<?xml version="1.0" encoding="UTF-8"?><?xml-stylesheet href="/scripts/pretty-feed-v3.xsl" type="text/xsl"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:h="http://www.w3.org/TR/html4/"><channel><title>Anthony J. G.</title><description>A Chaotic Digital Garden</description><link>https://anthony-j-g.github.io</link><item><title>Managing Bulky Binary Assets</title><link>https://anthony-j-g.github.io/garden/bin-mngmt</link><guid isPermaLink="true">https://anthony-j-g.github.io/garden/bin-mngmt</guid><description>Large binary files are a cost on any substantially sized project that requires them. Microsoft&apos;s Vcpkg manager presents an interesting possible solution.</description><pubDate>Thu, 26 Jun 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;During software development, being able to view of a project&apos;s history across time has many positive implications for compatibility, security, and just general learning. Because of the very nebulous nature of solving this problem and a relative necessity for project creators to solve this problem, there are a number solution platforms availible. Of these different kinds of solutions in the set, Git versioning is pretty ubiquitously considered the best local minimum across the problem domain.&lt;/p&gt;
&lt;h2&gt;Some Motivating Problems&lt;/h2&gt;
&lt;p&gt;One of the biggest points of friction for Git occurs when trying to integrate external assets into a project. This is problem rears its ugly head the most when it comes to game projects and any project that requires lots of art assets. This can come in the form of precompiled binary files, image files, databases, 3d models, and more. These file types are bulky, bloated, and are hard for Git to compare differences to.&lt;/p&gt;
&lt;p&gt;Git only saves the differences between multiple versions of the same file as opposed to the file in it&apos;s entirety multiple times to prevent from tracking redundant information. This means that a single file could be edited a huge number of times but still keep the &lt;code&gt;.git/&lt;/code&gt; folder that saves metadata for the project as lightweight as possible. The problem with binary files occurs due to the nature of how most binary files are created. Regardless of the data being saved, most often a small change in the data the binary file represents can make a massive change in the underlying structure of the binary. This means that tracking all of the possible information that the binary file can have results in a larger memory footprint inside of Git, and in turn gives the repository a larger memory footprint.&lt;/p&gt;
&lt;p&gt;Another corollary of storing assets directly in the version control system (VCS) is that it &lt;strong&gt;requires&lt;/strong&gt; artists who may or may not have any kind of real technical prowess to interact with the system. This creates a pretty heavy knowledge floor that in the best case requires the hiring of artists who already have knowledge of the underlying VCS technology. This could potentially lead to significant decrease in the set of talent that can be hired, and in the worst case could cause an artist who already &lt;strong&gt;has&lt;/strong&gt; experience with the system to sink unnecessary time in fighting the version control versus actually practicing their profession.&lt;/p&gt;
&lt;h2&gt;Odin: A Memory Footprint Case Study&lt;/h2&gt;
&lt;p&gt;A working example of this can be seen in the repository for the &lt;a href=&quot;https://github.com/odin-lang/Odin&quot;&gt;Odin programming language&lt;/a&gt;. The language repository commits external libraries, such as &lt;a href=&quot;https://github.com/odin-lang/Odin/blob/master/vendor/sdl3/SDL3.dll&quot;&gt;SDL3&lt;/a&gt; and &lt;a href=&quot;https://github.com/odin-lang/Odin/blob/master/vendor/raylib/windows/raylib.dll&quot;&gt;raylib&lt;/a&gt; directly into the source tree so that maintainers can have easy access to any necessary dependencies during development.&lt;/p&gt;
&lt;p&gt;The direct inclusion of vendored libraries provides a convenience for anyone who wants to get started either in development on the language or creating a new project using the language. This also follows closely with Odin&apos;s design philosophy for being simple with a focus on actually creating software projects instead of focusing on the framework supporting them.&lt;/p&gt;
&lt;p&gt;After cloning the &lt;a href=&quot;https://github.com/odin-lang/Odin/commit/62e797b9d15d32b7db906e99e98f0943bf2aa6e3&quot;&gt;repository with a given SHA hash for consitency&lt;/a&gt; and no further manipluation, Windows calculates the total size of the repository as &lt;code&gt;930 MB&lt;/code&gt; and the total size of the &lt;code&gt;.git/&lt;/code&gt; metadata folder as &lt;code&gt;520 MB&lt;/code&gt;. This is a percentage footprint of over 50%!&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;./assets/odin_breakdown-of-repo-mem-footprint_chart.png&quot; alt=&quot;Figure [odin-chart-1]: Memory footprint of different top-level folders of the repository&quot;&gt;&lt;/p&gt;
&lt;p&gt;We can further break this down to do an analysis on the Git metadata itself to see where most of this memory is accumulating. After running the following shell commands from inside the repository we can get an output of the different file sizes of each commit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git rev-list --objects --all --missing=print |
  git cat-file --batch-check=&apos;%(objecttype) %(objectname) %(objectsize) %(rest)&apos; |
  sed -n &apos;s/^blob //p&apos; |
  sort --numeric-sort --key=2 |
  cut -c 1-12,41- |
  $(command -v gnumfmt || echo numfmt) --field=2 --to=iec-i --suffix=B --padding=7 --round=nearest
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Putting aside the fact that this set of shell commands looks like pure arcane gibberish to anyone who lacks a pretty intimate knowledge of Git, the resulting output tells an interesting story. Virtually &lt;strong&gt;all&lt;/strong&gt; of the files that appear in the list that are greater than &lt;code&gt;1 MiB&lt;/code&gt; (outside of a few generated C source and header files) are bulky binary files. You see files like &lt;code&gt;bin/lld-link.exe&lt;/code&gt;, &lt;code&gt;wgpu_native.lib&lt;/code&gt;, and in particular &lt;code&gt;LLVM-C.dll&lt;/code&gt;, which are not only greater than &lt;code&gt;100 MiB&lt;/code&gt; each but also appear &lt;strong&gt;multiple times&lt;/strong&gt; in the repository&apos;s history. &lt;code&gt;LLVM-C.dll&lt;/code&gt; alone comprises of &lt;code&gt;719 MiB&lt;/code&gt; in total memory footprint. Compare this to a source file like &lt;code&gt;core/sys/windows/types.odin&lt;/code&gt; which appears drastically more times, 155 times to be exact, in the repository&apos;s history and only has a memory footprint of &lt;code&gt;14.126 MiB&lt;/code&gt;. It&apos;s important to note that these numbers don&apos;t reflect their size inside of the &lt;code&gt;.git/&lt;/code&gt; folder, but rather the total amount of &lt;a href=&quot;https://en.wikipedia.org/wiki/Information#Information_theory&quot;&gt;information&lt;/a&gt; necessary in total to encode them.&lt;/p&gt;
&lt;p&gt;This very short case study is not to say that this approach is incorrect in the slightest, Odin is an amazing language that accomplishes its goals very well. However, it is more to ask the question; what is the cost of this convenience? Moreover, it&apos;s to highlight the fact that &lt;strong&gt;Git is a solution addressing the source control problem that lives in a local minimum&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Supply Chain Attacks&lt;/h2&gt;
&lt;p&gt;Another weakness of direct inclusion appears in the security domain. This practice increases the probability that your repository is vulnerable to &lt;a href=&quot;https://en.wikipedia.org/wiki/XZ_Utils_backdoor#Mechanism&quot;&gt;supply chain attacks&lt;/a&gt;. I&apos;m no security expert, so I won&apos;t pretend to speak with any kind of authority on the matter, but the inclusion of binary files in a Git repository appears like it would increase the attack surface of the project. I&apos;d also hypothesize that this potential vulnerability is not limited to when your project has external contributors. Files types that are used in asset production, such as Blender&apos;s &lt;code&gt;.blend&lt;/code&gt; files, are binary files that sometimes require the use of 3rd party addons and plugins that an artist innocently uses to produce their work. These addons could possibly contain executable code that, when used, poison the project in conjunction with Git and GitHub&apos;s automation functionality.&lt;/p&gt;
&lt;p&gt;Such vulnerabilties within the scope of my limited knowledge are strictly theoretical. None the less, even the possiblity of their existence in an attack vector raises questions about the safety directly including binary files in a repository.&lt;/p&gt;
&lt;h2&gt;Where To Go From Here?&lt;/h2&gt;
&lt;p&gt;Git has a few different workarounds for this problem, one of which that has been developed relatively recently (development began sometime around 2014) is &lt;a href=&quot;https://git-lfs.com/&quot;&gt;Git LFS&lt;/a&gt;. LFS attempts to track the checksums of each file type the maintainer determines is adequate, along with some other metadata. This results in a hugely reduced memory footprint since that metadata is all stored in a plaintext format, and the large files exist solely on the server until required. While this extension to Git provides some interesting ideas, it still has one major flaw. Artists still have to interact with the VCS directly.&lt;/p&gt;
&lt;p&gt;To address the problem of vendoring external libraries for C/C++ projects, Microsoft created the package management tool &lt;a href=&quot;https://vcpkg.io/en/&quot;&gt;vcpkg&lt;/a&gt;. Vcpkg provides an interface for client users to fetch, build, and vendor 3rd party libraries through two simple configuration files and a relatively digestable CLI. This interface also supports vendoring libraries on multiple platforms, so client users can aqcuire the pacakges they need regardless of the platform or CPU architecture they are operating on. Vcpkg&apos;s core tenents are about improving the C/C++ build pipeline to make any pipeline better at supporting cross-platform compilation as well as adding external binary libraries to any possible build and project system. Taken directly from the project&apos;s overview page in the official documentation, one of its explicit goals is to &quot;Build dependencies from source or download prebuilt ABI-verified binaries, with over 70 configurations available by default, and infinite customization for your specific requirements&quot;.&lt;/p&gt;
&lt;p&gt;At this point, you might be confused as to how a C/C++ build tool solves, in part or in full, the bulky binary file sub-problem of version control. The answer also comes from the &lt;a href=&quot;https://vcpkg.io/en/&quot;&gt;vcpkg homepage&lt;/a&gt; as of the time of writing, allowing users to &quot;add your own private libraries to simplify your build process&quot;. Precompiled C/C++ libraries, like the ones that vcpkg is designed to support by default, are just binary files. Should custom vcpkg registries provide a configurable enough interface, we should be enabled to vendor &lt;em&gt;any&lt;/em&gt; arbitrary binary file. As it turns out, this is the case. Inside our project&apos;s &lt;code&gt;vcpkg_configuration.json&lt;/code&gt;, all we need to do is specify a reference to where the registry exists and vcpkg takes care of the rest. Even more importantly, this reference can be &lt;a href=&quot;https://learn.microsoft.com/en-us/vcpkg/users/authentication&quot;&gt;a registry that is not publicly available&lt;/a&gt; and we can still utilize vcpkg&apos;s functionality.&lt;/p&gt;
&lt;p&gt;The custom registry itself comprises of configuration code that instructs vcpkg of what to do when a client tries to vendor a specifc &quot;port&quot;, where a port is a collection of binary assets the client wants to use. Client users and companies can define a port for a variety of use cases. It can be a character asset collection, pre-compiled libraries, texture libraries, pre-compiled shaders, and more. All the client user needs to do is implemnt the &lt;code&gt;portfile.cmake&lt;/code&gt; and &lt;code&gt;vcpkg.json&lt;/code&gt; configurations for their custom port and vcpkg will take care of the rest when a developer invokes &lt;code&gt;vcpkg install&lt;/code&gt; from inside the project.&lt;/p&gt;
&lt;p&gt;Crucially, ports can reference almost any type of endpoint. That endpoint can be a server where all an artist needs to do is upload their work and not even need to have a single line of code on their computer to be able to keep contributing to the project. Custom registries can also define any amount of ports, allowing a single registry to be used &lt;strong&gt;across&lt;/strong&gt; projects, providing a company or team with very granular reusablity.&lt;/p&gt;
&lt;h2&gt;Some Closing Remarks&lt;/h2&gt;
&lt;p&gt;It should be important to note that this is &lt;strong&gt;not an advertisement for vcpkg&lt;/strong&gt;. There are a bunch of glaring problems with using it in this way, or even using it in general. Most notably, the fact that its an open source Microsoft project, a company that famously has subscribed to the motto of &lt;a href=&quot;https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish#Examples_by_Microsoft&quot;&gt;&quot;Embrace, extend, and extinguish&quot;&lt;/a&gt; in the past. Additionally, the heart of registry configuration is done in CMake. Even though Microsoft provides &lt;a href=&quot;https://learn.microsoft.com/en-us/vcpkg/examples/packaging-github-repos#suggested-example-portfiles&quot;&gt;examples for vcpkg ports that don&apos;t use CMake&lt;/a&gt; as their build system, anyone who has done any kind of C/C++ development using CMake should be intimately aware of why this could be a problem.&lt;/p&gt;
&lt;p&gt;Instead of purely advocating for the use of vcpkg as &lt;strong&gt;the&lt;/strong&gt; solution, this train of thought is more so designed to think about a way to solve this bulky binary management sub-problem of Git. It is also attempting to avoid the work of writing a custom VCS from scratch and instead use Git as a bootstrap towards a more optimal minimum.&lt;/p&gt;</content:encoded><h:img src="/_astro/calling-zig-from-c.DzZDiYKA.jpg"/><enclosure url="/_astro/calling-zig-from-c.DzZDiYKA.jpg"/></item><item><title>A Rudimentary Planetary Temperature Model</title><link>https://anthony-j-g.github.io/garden/planet-temp-gradient-model</link><guid isPermaLink="true">https://anthony-j-g.github.io/garden/planet-temp-gradient-model</guid><description>Using blackbody radiation and light&apos;s interaction with matter, we can create a rudimentary model of planetary temperature and atmosphere</description><pubDate>Fri, 12 Nov 2021 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Planetary temperature is a broad topic that, like many other thermodynamic problems, contains a deep level of complexity. In spite of this, we can still create a simple and illustrative model predicated on the observation that the total energy within a system can only be changed through energy entering or leaving the system. As stated relative to the context of specifically the &lt;a href=&quot;https://en.wikipedia.org/wiki/Laws_of_thermodynamics#First_law&quot;&gt;First Law of Thermodynaimcs&lt;/a&gt;:
$$
\Delta U_{system} = Q + W
$$&lt;/p&gt;
&lt;p&gt;In this context, $\Delta U_{system}$ is a measurement of the change of energy in the system, while $Q$ and $W$ are measurements of the amount of energy entering in the system due to &lt;strong&gt;heat&lt;/strong&gt; and &lt;strong&gt;work&lt;/strong&gt; respectively. One of the properties of temperature is that it is directly proportional to the energy content of a system. So then as the energy content increases, so does the temperature. But what does this mean specifically in regards to our simple planetary model?&lt;/p&gt;
&lt;h2&gt;Blackbody Spectra&lt;/h2&gt;
&lt;p&gt;For any arbitrary planet, there are really only 2 main sources of energy, photons emitted by nearby stellar objects and internal thermal radiation from the planet&apos;s core. For Earth, the first source would be the light emitted by our Sun. But how much of the light from our sun actually reaches us? And how can we quantify that? This is where the concept of a &lt;strong&gt;blackbody&lt;/strong&gt; comes in. Denotatively, a blackbody is an &quot;idealized physical body that absorbs all incident electromagnetic radiation, regardless of frequency or angle of incidence&quot;. This characteristic absorption of light causes the object either heat up or glow to eject excess energy. For solid objects, this causes them to glow and emit a spectrum that depends only on temperature and provides a way for an observer to measure the underlying temperature of the object. In 1798, Josef Stefan empirically measured the relationship between the temperature of a blackbody and it&apos;s corresponding emission of energy as,&lt;/p&gt;
&lt;p&gt;$$
L = A \sigma T^4
$$&lt;/p&gt;
&lt;p&gt;where $T$ is temperature of the blackbody, $A$ is it&apos;s surface area, and $\sigma$ is referred to as the &lt;strong&gt;Stefan-Boltzmann constant&lt;/strong&gt;, named after Stefan and the physicist Ludwig Boltzmann who proved the relationship theoretically. The resulting quantity is referred to as the blackbody&apos;s &lt;strong&gt;luminosity&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;At this point, it&apos;s important to note that because blackbodies are strictly theoretical objects, these results are not equivalent to the actual characteristics of the underlying object. However, they are mathematical tools that prove to be accurate enough to give reliable estimation and create working models.&lt;/p&gt;
&lt;h2&gt;Estimating the Spectrum and Finding Colors&lt;/h2&gt;
&lt;p&gt;Using blackbody objects as mathematical tools, we can then define it&apos;s wavelength spectrum to be $B_\lambda(\lambda ; T)$. This function determines what the intensity of each wavelength, visible light, infrared, etc., would be for a given input temperature. The physicist Max Planck used the total luminosity relationship defined above as well as the following integral of $B_\lambda(\lambda ; T)$ to derive a formulation of it.
$$
L = \int_{0}^{\infty}{d\lambda} \int{dA} \int_{0}^{2\pi}{d\phi} \int_{0}^{\frac{\pi}{2}}{d\theta} \cos{\theta} \sin{\theta} B_\lambda(\lambda ; T)
$$&lt;/p&gt;
&lt;p&gt;And while this is integral to the final derivation (pun intended), the final result that Planck achieved for the wavelength spectrum is;
$$
B_\lambda(\lambda ; T) = \frac{2hc^2}{\lambda^5}\frac{1}{e^{hc/\lambda kT} - 1}
$$&lt;/p&gt;
&lt;p&gt;Achieving this result from the integral is left as an exercise to the reader. Using the wavelength spectrum we can see a few predicions of what wavelengths might appear across different temperatures.&lt;/p&gt;</content:encoded><h:img src="/_astro/nasa-Q1p7bh3SHj8-unsplash.pVPH3inX.jpg"/><enclosure url="/_astro/nasa-Q1p7bh3SHj8-unsplash.pVPH3inX.jpg"/></item></channel></rss>